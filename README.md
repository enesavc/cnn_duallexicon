# cnn_duallexicon
This Repository contains the sample code or scripts used in the CNN Dual Lexicon project developed by the [Gow Lab](https://gowlab.mgh.harvard.edu/), Department of Neurology, Massachusetts General Hospital/Harvard Medical School.

## CNN Dual Lexicon Preprint

Avcu, E., Newman, O., Gow, D. (2021). A Tale of Two Lexica: Testing Computational Hypotheses with Deep Convolutional Neural Networks. arXiv preprint arXiv:XXXXXX.XXXXXX.

## Link to Talks

### Link to our Society of Neurobiology of Language Poster Presentation
https://www.youtube.com/watch?v=nfQbnTl9hbY

### Link to our Psychonomics Society Talk
https://www.youtube.com/watch?v=mfVCsV8wSOg

## Environment Specs
The following specs are for a CentOS Linux system.
It is suggested to set up a conda environment (optional), activate your environment, install python, tensorflow, or anyother packages into your specific environment.

Example Code
```
conda create --prefix ./ns/gpu
conda activate /PATH/ns/gpu
conda install --prefix ./gpu python=3.8.5
conda install --prefix ./gpu tensorflow-gpu==2.4.1
```

## Step 1: Preparing the Training Data
We have used the [Spoken Wikipedia Corpus](https://nats.gitlab.io/swc/) (SWC) to extract the audio files. We used words that has at least four characters long and occurred between 200 and 450 times. We have used 178 words for this preliminary task.
We extracted two second clips for each occurrence of a target word and these two second clips were mixed with three different background noises (three different noise) with randomly assigned SNR levels (see the preprint paper for the details of these process). The noise files were generated by using the following repositories: (i) auditory scenes, (ii) instrumental music, (iii) multi-speaker speech babble. We used the 2013 IEEE AASP [Challenge on Detection and Classification of Acoustic Scenes and Events corpus](http://c4dm.eecs.qmul.ac.uk/sceneseventschallenge/description.html)  (Stowell et al., 2015) for auditory scenes and corpus of public domain audiobook recordings (https://librivox.org/) to create the multi speaker speech babble. For the instrumental music, we used the Instrument Recognition in Musical Audio Signals (IRMAS) corpus (Bosch et al., 2012) which  includes predominant instruments like cello, clarinet, flute, acoustic guitar, electric guitar, organ, piano, saxophone, trumpet, violin.

### Command
```
python extract_words.py
```

## Step 2: Cochleagram Creation
We used cochleagrams of each two second clip as the input to the network. A cochleagram is very similar to a spectrogram which is used to represent audio signals in the time frequency domain. Cochleagrams were created using the code shared by [Feather et. al. (2019)](https://github.com/jenellefeather/tfcochleagram). Each two second clip was passed through a bank of 203 bandpass filters resulting a cochleagram representation of 203 x 400 (frequency x time). See figure below for a schematic representation of training data preparation.

### Command
```
python tfcochleagram_ns.py
```
![image](https://user-images.githubusercontent.com/32641692/112358864-17b38480-8ca7-11eb-8489-323c2792469a.png)


